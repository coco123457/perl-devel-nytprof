# vim: ts=2 sw=2 sts=0 noexpandtab:
# $Id$

HACKING Devel::NYTProf
======================

The New York Times and I encourage hacking Devel::NYTProf!

OBTAINING THE CURRENT RELEASE
-----------------------------
The current offial release can be obtained by two methods.

The CPAN Page (Preferred):
http://search.cpan.org/~akaplan/Devel-NYTProf/

The Google Code page:
http://code.google.com/p/perl-devel-nytprof/downloads/list

OBTAINING THE LATEST DEVELOPMENT CODE
-------------------------------------
You can grab the head of the latest trunk code from the Google Code repository:
http://code.google.com/p/perl-devel-nytprof/source/checkout

or directly via SVN:

svn checkout http://perl-devel-nytprof.googlecode.com/svn/trunk/ perl-devel-nytprof-read-only

CONTRIBUTING
------------
Small patches are accepted via email (akaplan@nytimes.com) or Google Group post.

Larger changes will need SVN write-access.  You can obtain this by emailing
Adam (akaplan@nytimes.com).  You'll need to have a Google account first, so go
make one first.  Include the username in your request for access.  After access
is obtained, you can use this command:

svn checkout https://perl-devel-nytprof.googlecode.com/svn/trunk/ perl-devel-nytprof --username YOUR_USERNAME

COMPILING
---------
When developing, use 'make' and ensure that NO warnings are output except
for those occuring in perl "XS_blah_blah" functions.

Change line 37 of Makefile from:

CCCDLFLAGS = -fPIC

to:

CCCDLFLAGS = -fPIC -pedantic -Wall -ansi

TESTING
-------
You MUST write test cases for you changes. All tests that are dropped into the
"t" folder and added to MANIFEST will be executed.  The testing system is
customized for this module becuse debuggers are not that easy to test.  The
system still uses Test::Harness and Test::More, so it should behave just like
any other perl modules 'make test'.

Writing tests is easy!

1) design a perl script that will trigger the new behavior/feature that you
   want to test. Name the file 't/test##.p'

2) create the 'control' file for the debugger output.  This file contains a
   perl fragment that is eval'd.  See any t/*.v file for an example, but
   basically it should be a hash of: line_number => [ times_executed ].
   Time data and statistics are stripped out. They aren't reliable metrics
   across systems.  If you are testing an eval, then you'll need to make an
   anonymous hash that hash the lines IN the eval that were executed. The format
   is the same. Examples are in t/test13.v, but it looks like this:

	 $expected = {
			'test00.p' => {
				4 => [ 3 ],
				19 => [ 1, { 1 => [ 1 ] } ],
				20 => [ 2, { 1 => [ 1 ], 2 => [ 100 ] } ],
			}
	 };

	 This hash is for the files "t/test00.p"
	 Line 4 is executed 3 times
	 Line 19 is executed 1 time and is an eval
	      The first line of the eval is executed 1 time
	 Line 20 is executed 1 time and is also eval
	      The first line of the eval is executed 1 time
	      The second line of the eval is executed 100 times
	
	 Evals in the debugger are strange, and you may need to do some exploring if
	 you manage to create a strange eval.  If code in an eval fails during test,
	 subsequent lines INSIDE the eval are reported as having NEVER executed!

3) Create a corresponding CSV output file.  This is EASY! The file will be
	 compared against the output of bin/nytprofcsv on your test file.  It tests
	 the output generator code.  Everything except number of calls should be set
	 to 0.  Times are ignored, but there is a way to override that. Look through
	 the test files to find some examples of that. Files are named t/test00.x

	 Note: This is a very easy step! Just run "make test".  The scripts will fail
	 when trying to verify the csv file, but the file that it generated can be
	 copy-pasted from t/profiler/blah.p.csv.  You'll need to verify it of course!

4) Test! run 'make test' and your new paried .p,.x & .v files will be tested

Note:  While writing a test, it is helpful to be able to run it directly, 
			without the test harness.  This allows you to view more output stdout and 
			stderr.  Fortunately, its easy to do:

			PERL5LIB=blib/lib:blib/arch perl -d:NYTProf t/test01.p

			The output will be in ./nytprof. You can then also run
			the csv manually:

			PERL5LIB=blib/lib:blib/arch perl bin/nytprofcsv

			The final file will be in ./profiler/test01.p.csv

Remember, testing is VERY VERY important!  Within a day or two of releasing
code, the CPAN testers will test the release on pretty much every major platform
you can think of.  A failed test report is much easier to fix than a runtime
error like "bash: segmentation fault: core dumped"

GENERATING DISTRIBUTIONS
------------------------
Releases are generated with 'make metafile', and then fed through tar+gz.
You shouldn't ever check-in the distribution directory, any temporary files
(including Makefile.old) or change the $VERSION numbers. We'll do this for you.

RESOURCES
---------
Google Code:
http://code.google.com/p/perl-devel-nytprof/

Google Devel Group (must subscribe here):
http://groups.google.com/group/develnytprof-dev

NYTimes Open Code Blog:
http://open.nytimes.com/

TODO
----

Fix Reader
- not document methods with a leading underscore
- to have consistent_naming_style notMixedCamelCase (fixed?)
- to be fully OO (ie not document non-OO interfaces)
- to be subclassable
- to provide a subclass to manage generating CSV
- to provide a subclass to manage generating HTML
Then rework bin/ntyprof* to use the new subclasses
Ideally end up with a single nytprof command.

Add (very) basic nytprofhtml test (ie it runs and produces output)

Rework option parsing so options can be implemented in perl, accessed from
perl, and stored in data file.

Write tests for new functionality.

Write up change notes

Write, clean-up documentation

Ability to run bin/nytprof* in perl -d (debugger mode)

Currently the cost of calling the sub goes to the caller (good)
but the cost of copying the result goes to the method (poor).
Overriding pp_leavesub may allow more accurate measurements by avoiding return
value processing overheads accumulating on last statement in sub.

Similarly, the cost of retesting the loop condition is accounted
for by the last statement executed each time round the loop.
Overriding relevant ops may enable better accounting.

Add way for program being profiled to switch output to a new profile file.
Perhaps via enable_profile($optional_new_filename)
See http://search.cpan.org/dist/Devel-Profile/ for use case.

Add @INC to data file so reports can be made more readable by removing
(possibly very long) library paths where appropriate.

Add size and mtime of fid to data file.

Intercept all opcodes that may fork and run perl code in the child
	ie fork, open, entersub (ie xs), others?
	and fflush before executing the op (so fpurge isn't strictly required)
  and reinit_if_forked() afterwards
	add option to force reinit_if_forked check per stmt just-in-case

Add way to merge profile data. Merging could be done in perl.

Add constants to Data.pm for the array indexes
0=time_spent
1=exe_count
2=eval_line_data

Add method to remove data (like sub_fid_lines) relating to specific files
so that the .rdt tests aren't affected by variations between versions of library modules.

Support profiling programs which use threads:
	- move all relevant globals into a structure
	- add lock around output to file

Add functions to read & write strings to the profile data file that uses an
integer length prefix.  Replace all current newline-terminated strings with the
new code, so we're safe from embedded newlines and can also store multi-line strings.

Support optionally generating a compressed data file (using popen("zip -c > $file"))

Support optionally saving eval strings (from @{"_<$filename"}, see perldoc perldebguts)
Should always save the first eval string at each distinct eval (fid:line)
and require an option to save them all (which could make the data file very large).

Add % of total time to index page
Remove bottom stats.

Use clock_gettime(CLOCK_MONOTONIC) if available, else clock_gettime(CLOCK_REALTIME).
Those calls are cheaper than gettimeofday and can give nanosecond resolution.

nytprofhtml: output list of subs as a table and include timings aggregated from
the line ranges. (Won't be accurate for overlapping subs, such as anonymous
subs, but those can be detetected and a note added.)

The whole reporting framework needs a rewrite to use a single 'thin' command
line and classes for the Model (lines, files, subs), View (html, csv etc),
and Controller (composing views to form reports).

Add timing to sub calls by using save stack to trigger end of timing measurement?

Add resolution of __ANON__ sub names (eg imported 'constants') where possible.

Trim leading @INC portion from filename in __ANON__[/very/long/path/...]
in report output. (Keep full path in link/tooltip/title as it may be ambiguous when shortened).

In "$sub called:\n" show total count of calls: "$sub called $count times:"

Explain what's shown in html reports, ie say it's elapsed realtime.

Use 'Statements' or 'Stmts' instead of Calls in reports.

Only warn "No file line range data for sub' once per sub.
Add hint that it may be XS.

Rename Foo::BEGIN subs to Foo::BEGIN[file:line]
(which matches the style used for Foo::__AUTO__[file:line])

Refactor this HACKING file!

Add docs for new options

Add docs for use_db_sub and explain pros & cons. Especially that opcode
redirection can't profile code that was loaded before NYTProf because
it only affects code that is *compiled* after it takes effect.
(Unless we want to traverse the existing compiled code and edit the ops in place :-)

Would be good if DB::DB sub wasn't defined when use_db_sub is false
so the profiler could be used to profile debuggers etc.
